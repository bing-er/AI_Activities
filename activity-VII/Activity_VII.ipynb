{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_n91VbEff_T"
   },
   "source": [
    "# Week 8: Computer Vision III ‚Äî Object Detection\n",
    "## COMP 9130 ‚Äî Applied Artificial Intelligence\n",
    "## **Student Activity Notebook**\n",
    "\n",
    "---\n",
    "\n",
    "**Business Context:** CityView Traffic Analytics ‚Äî a municipal transportation department building an automated traffic monitoring system to analyze intersection safety, count vehicles, and detect pedestrians.\n",
    "\n",
    "**Tool:** YOLO26 via Ultralytics\n",
    "\n",
    "**Task 1 Dataset:** Sample traffic scene images + pre-trained YOLO26 (COCO weights ‚Äî already detects cars, trucks, buses, pedestrians, traffic lights, stop signs)\n",
    "\n",
    "**Task 2‚Äì3 Dataset:** Road sign detection dataset from Roboflow (~700 images, 4 classes: traffic light, stop, speed limit, crosswalk)\n",
    "\n",
    "**‚è∞ TIMING GUIDE:**\n",
    "| Time | Activity | Points |\n",
    "|------|----------|--------|\n",
    "| 0:00‚Äì0:20 | Quiz 7 (Object Detection Concepts from Prep) | 5 pts |\n",
    "| 0:20‚Äì0:30 | Setup & Installation | ‚Äî |\n",
    "| 0:30‚Äì1:20 | Task 1: Pre-trained YOLO for Traffic Monitoring | 5 pts |\n",
    "| 1:20‚Äì1:30 | Break | ‚Äî |\n",
    "| 1:30‚Äì2:20 | Task 2: Explore Object Detection Datasets & Annotations | 5 pts |\n",
    "| 2:20‚Äì2:55 | Task 3: Fine-tune YOLO26 on Custom Data | 5 pts |\n",
    "| 2:55‚Äì3:00 | Wrap-up & Mini Project 7 Introduction | ‚Äî |\n",
    "\n",
    "**üéØ KEY LEARNING GOALS:**\n",
    "1. Understand the difference between classification, detection, and segmentation\n",
    "2. Use a pre-trained YOLO model for inference on real images\n",
    "3. Understand YOLO annotation format (class, x_center, y_center, width, height)\n",
    "4. Fine-tune YOLO26 on a custom dataset and evaluate with IoU and mAP\n",
    "\n",
    "**‚ö†Ô∏è COMMON STUDENT STRUGGLES:**\n",
    "1. Confusing classification (one label per image) vs. detection (multiple boxes per image)\n",
    "2. YOLO annotation format ‚Äî normalized coordinates, center-based, one .txt per image\n",
    "3. Understanding confidence thresholds ‚Äî too high misses objects, too low gives false positives\n",
    "4. IoU concept ‚Äî students may confuse it with accuracy\n",
    "5. mAP calculation ‚Äî it's averaged over IoU thresholds AND classes\n",
    "6. Dataset directory structure ‚Äî YOLO expects images/ and labels/ folders in specific layout\n",
    "\n",
    "**üìå KEY DIFFERENCE FROM WEEKS 6‚Äì7:**\n",
    "- Weeks 6‚Äì7: Classification = one label per image, trained with Keras/TensorFlow\n",
    "- Week 8: Detection = multiple bounding boxes per image, trained with Ultralytics/PyTorch\n",
    "- Students will notice the API is completely different ‚Äî that's intentional!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZHBPoETff_X"
   },
   "source": [
    "## Setup & Installation\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** If students see warnings about `albumentations` or `wandb`, those are optional and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xso5F1fsff_Y",
    "outputId": "470f7571-3638-46e5-9220-f2d15f5fc2d9"
   },
   "source": [
    "# ============================================\n",
    "# SETUP & INSTALLATION\n",
    "# ============================================\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q ultralytics roboflow\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# YOLO imports\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Verify environment\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected ‚Äî training will be slow!\")\n",
    "    print(\"Go to: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTW-HNCsff_Z"
   },
   "source": [
    "---\n",
    "\n",
    "# Task 1: Pre-trained YOLO for Traffic Monitoring\n",
    "\n",
    "**Key concepts to emphasize:**\n",
    "1. YOLO = \"You Only Look Once\" ‚Äî processes the entire image in one forward pass\n",
    "2. Each detection has: bounding box (x, y, w, h), class label, confidence score\n",
    "3. The model outputs MANY candidate boxes ‚Üí Non-Maximum Suppression (NMS) filters them\n",
    "4. Confidence threshold controls the trade-off between precision and recall\n",
    "\n",
    "---\n",
    "\n",
    "## Part A: Load Pre-trained YOLO26"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "yxVvneIJff_Z",
    "outputId": "67d07178-983f-4cff-bc13-8826e8cd03ea"
   },
   "source": [
    "# ============================================\n",
    "# LOAD PRE-TRAINED YOLO26 MODEL\n",
    "# ============================================\n",
    "\n",
    "# yolo26n (nano)    ‚Äî 2.4M params, fastest, least accurate\n",
    "# yolo26s (small)   ‚Äî 9.5M params\n",
    "# yolo26m (medium)  ‚Äî 20.4M params\n",
    "# yolo26l (large)   ‚Äî 25.3M params\n",
    "# yolo26x (xlarge)  ‚Äî 59.1M params, slowest, most accurate\n",
    "# We use 'n' (nano) for speed in class.\n",
    "\n",
    "model_pretrained = YOLO('yolo26n.pt')  # Downloads automatically\n",
    "\n",
    "# Explore model info\n",
    "print(\"üìã Model Summary:\")\n",
    "print(f\"  Model type: YOLO26n (Nano)\")\n",
    "print(f\"  Task: Object Detection\")\n",
    "print(f\"  Number of classes: {len(model_pretrained.names)}\")\n",
    "print(f\"\\nüè∑Ô∏è COCO Classes (first 20):\")\n",
    "for i, name in list(model_pretrained.names.items())[:20]:\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "# Show traffic-relevant classes\n",
    "traffic_classes = ['car', 'truck', 'bus', 'motorcycle', 'bicycle',\n",
    "                   'person', 'traffic light', 'stop sign']\n",
    "print(f\"\\nüöó Traffic-relevant COCO classes:\")\n",
    "for name in traffic_classes:\n",
    "    class_id = [k for k, v in model_pretrained.names.items() if v == name]\n",
    "    if class_id:\n",
    "        print(f\"  Class {class_id[0]}: {name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUE7OaEnff_Z"
   },
   "source": [
    "## Part B: Download Sample Traffic Images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "id": "oTFLWaEHff_Z",
    "outputId": "8ee9b5d0-9421-4c4f-f264-2290b9a2f6af"
   },
   "source": [
    "# ============================================\n",
    "# DOWNLOAD SAMPLE TRAFFIC IMAGES\n",
    "# ============================================\n",
    "\n",
    "# These reliably download and contain traffic-relevant objects.\n",
    "\n",
    "os.makedirs('sample_images', exist_ok=True)\n",
    "\n",
    "sample_urls = [\n",
    "    \"https://ultralytics.com/images/bus.jpg\",\n",
    "    \"https://ultralytics.com/images/zidane.jpg\",\n",
    "]\n",
    "\n",
    "import urllib.request\n",
    "downloaded_images = []\n",
    "for url in sample_urls:\n",
    "    filename = os.path.join('sample_images', os.path.basename(url))\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        downloaded_images.append(filename)\n",
    "        print(f\"‚úÖ Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not download {url}: {e}\")\n",
    "\n",
    "# Display the sample images\n",
    "fig, axes = plt.subplots(1, len(downloaded_images),\n",
    "                          figsize=(7 * len(downloaded_images), 7))\n",
    "if len(downloaded_images) == 1:\n",
    "    axes = [axes]\n",
    "for ax, img_path in zip(axes, downloaded_images):\n",
    "    img = Image.open(img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(os.path.basename(img_path), fontsize=12)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Images (Before Detection)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9wBktuJff_Z"
   },
   "source": [
    "## Part C: Run Detection & Visualize Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2029
    },
    "id": "ic4D7SmIff_Z",
    "outputId": "b08021fa-33ae-4707-f5c0-ccd1937a43ab"
   },
   "source": [
    "# ============================================\n",
    "# RUN DETECTION ON SAMPLE IMAGES\n",
    "# ============================================\n",
    "\n",
    "# Each result contains boxes, confidence scores, and class IDs.\n",
    "# conf=0.25 means only show detections with >25% confidence.\n",
    "\n",
    "results = model_pretrained(downloaded_images, conf=0.25)\n",
    "\n",
    "# Display results with detailed breakdown\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Image: {os.path.basename(downloaded_images[i])}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    boxes = result.boxes\n",
    "    print(f\"\\nüì¶ Detections found: {len(boxes)}\")\n",
    "    print(f\"{'‚îÄ' * 55}\")\n",
    "    print(f\"{'Class':<20} {'Confidence':<12} {'Box (x1,y1,x2,y2)'}\")\n",
    "    print(f\"{'‚îÄ' * 55}\")\n",
    "\n",
    "    for box in boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        xyxy = box.xyxy[0].cpu().numpy()\n",
    "        class_name = model_pretrained.names[cls_id]\n",
    "        print(f\"{class_name:<20} {conf:<12.3f} \"\n",
    "              f\"[{xyxy[0]:.0f}, {xyxy[1]:.0f}, {xyxy[2]:.0f}, {xyxy[3]:.0f}]\")\n",
    "\n",
    "    # Plot with bounding boxes\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    annotated = result.plot()  # Returns BGR numpy array\n",
    "    ax.imshow(annotated[..., ::-1])  # Convert BGR to RGB\n",
    "    ax.set_title(f\"Detections: {os.path.basename(downloaded_images[i])}\",\n",
    "                 fontsize=14)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MQ4pf1Zff_a"
   },
   "source": [
    "## Part D: Explore Confidence Thresholds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "I2cjW7ASff_a",
    "outputId": "3c9b216d-191f-464c-fb8c-9d6581c8ae73"
   },
   "source": [
    "# ============================================\n",
    "# CONFIDENCE THRESHOLD COMPARISON\n",
    "# ============================================\n",
    "\n",
    "test_image = downloaded_images[0]  # Use the bus image\n",
    "thresholds = [0.10, 0.25, 0.50, 0.75]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "\n",
    "for ax, thresh in zip(axes, thresholds):\n",
    "    result = model_pretrained(test_image, conf=thresh, verbose=False)\n",
    "    annotated = result[0].plot()\n",
    "    ax.imshow(annotated[..., ::-1])\n",
    "    n_detections = len(result[0].boxes)\n",
    "    ax.set_title(f\"Confidence ‚â• {thresh}\\n({n_detections} detections)\",\n",
    "                 fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Effect of Confidence Threshold on Detection Count',\n",
    "             fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight for CityView:\")\n",
    "print(\"  Low threshold (0.10): More detections, some false positives\")\n",
    "print(\"  High threshold (0.75): Fewer detections, but very confident\")\n",
    "print(\"  ‚Ä¢ Pedestrian safety ‚Üí lower threshold (don't miss anyone!)\")\n",
    "print(\"  ‚Ä¢ Vehicle counting ‚Üí higher threshold (accuracy matters more)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQz9n_eFff_a"
   },
   "source": [
    "## Part E: Filter for Traffic-Relevant Classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "nOxXnHtdff_a",
    "outputId": "131c0f70-f408-4c74-bd35-bcda3933cfdf"
   },
   "source": [
    "# ============================================\n",
    "# FILTER DETECTIONS BY CLASS\n",
    "# ============================================\n",
    "\n",
    "# COCO class IDs for traffic objects\n",
    "traffic_class_ids = [0, 1, 2, 3, 5, 7, 9, 11]\n",
    "# 0=person, 1=bicycle, 2=car, 3=motorcycle, 5=bus, 7=truck,\n",
    "# 9=traffic light, 11=stop sign\n",
    "\n",
    "# Compare filtered vs unfiltered\n",
    "result_all = model_pretrained(test_image, conf=0.25, verbose=False)\n",
    "result_traffic = model_pretrained(test_image, conf=0.25,\n",
    "                                   classes=traffic_class_ids,\n",
    "                                   verbose=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "axes[0].imshow(result_all[0].plot()[..., ::-1])\n",
    "axes[0].set_title(f\"All COCO classes ({len(result_all[0].boxes)} detections)\",\n",
    "                   fontsize=13)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(result_traffic[0].plot()[..., ::-1])\n",
    "axes[1].set_title(f\"Traffic classes only ({len(result_traffic[0].boxes)} \"\n",
    "                   f\"detections)\", fontsize=13)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle('CityView: Filtering for Traffic-Relevant Objects', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Traffic counting report\n",
    "print(\"\\nüìä CityView Traffic Report:\")\n",
    "print(\"=\" * 40)\n",
    "class_counts = Counter()\n",
    "for box in result_traffic[0].boxes:\n",
    "    cls_name = model_pretrained.names[int(box.cls[0])]\n",
    "    class_counts[cls_name] += 1\n",
    "for cls_name, count in class_counts.most_common():\n",
    "    print(f\"  {cls_name}: {count}\")\n",
    "print(f\"  Total traffic objects: {sum(class_counts.values())}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8TUtm6Jff_a"
   },
   "source": [
    "‚úÖ Task 1 Analysis\n",
    "Answer these questions based on YOUR detection results:\n",
    "\n",
    "Q1. How many objects did YOLO26 detect in the bus image at conf=0.25? List the classes and counts.\n",
    "\n",
    "5 objects, 1 bus and 4 peoples\n",
    "\n",
    "Q2. When you changed the confidence threshold from 0.10 to 0.75, how did the number of detections change? Relate this to the precision-recall trade-off from Week 3.\n",
    "\n",
    "Low threshold (0.10): More detections, some false positives\n",
    "\n",
    "High threshold (0.75): Fewer detections, but very confident\n",
    "\n",
    "Q3. For CityView's pedestrian safety application, would you use a high or low confidence threshold? Why?\n",
    "\n",
    "Pedestrian safety ‚Üí lower threshold (don't miss anyone!)\n",
    "\n",
    "Q4. Name at least 3 differences between image classification (Weeks 6‚Äì7) and object detection (today).\n",
    "\n",
    "output format - image classification outputs a single class probability distribution for the entire image. Ojbect detecction outpus\n",
    "number of objects - lassification generally assigns one primary label to an image (or a set of labels to the whole scene). Detection identifies, separates, and counts multiple individual objects within the exact same image.\n",
    "spatial information (localization) - Classification tells you what is in the image but not where it is. Object detection provides exact spatial localization, drawing a boundary around the exact pixels the object occupies.\n",
    "Call instructor to verify completion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLn25Eg4ff_a"
   },
   "source": [
    "---\n",
    "\n",
    "# Task 2: Explore Object Detection Datasets & Annotations\n",
    "\n",
    "**Key concepts to emphasize:**\n",
    "1. YOLO format: each image has a matching .txt file with one line per object\n",
    "2. Each line: `class_id x_center y_center width height` (all normalized 0‚Äì1)\n",
    "3. Dataset must be split into train/val (and optionally test)\n",
    "4. data.yaml tells YOLO where to find images/labels and what classes exist\n",
    "\n",
    "---\n",
    "\n",
    "## Part A: Download Road Sign Dataset from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0PN8Bo4xff_a",
    "outputId": "09995a76-01d1-4c44-888f-6c02e725347d"
   },
   "source": [
    "# ============================================\n",
    "# DOWNLOAD ROAD SIGN DATASET\n",
    "# ============================================\n",
    "\n",
    "# road sign detection dataset in YOLO format.\n",
    "#\n",
    "# SETUP BEFORE CLASS:\n",
    "# 1. Go to https://app.roboflow.com ‚Üí sign up (free)\n",
    "# 2. Go to Settings ‚Üí API Keys ‚Üí copy your key\n",
    "# 3. Replace the API_KEY below\n",
    "# 4. Dataset: Road Signs from RF100 benchmark (public)\n",
    "\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# ‚ö†Ô∏è INSTRUCTOR: Replace with your Roboflow API key\n",
    "ROBOFLOW_API_KEY = \"EPeijRTxqpNY9q8JNtH1\"\n",
    "\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "project = rf.workspace(\"roboflow-100\").project(\"road-signs-6ih4y\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "DATASET_DIR = dataset.location\n",
    "print(f\"\\n‚úÖ Dataset downloaded to: {DATASET_DIR}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hi9nCapsff_b"
   },
   "source": [
    "# ============================================\n",
    "# FALLBACK: If Roboflow is unavailable\n",
    "# ============================================\n",
    "\n",
    "# This downloads from a backup URL or uses a local copy.\n",
    "\n",
    "# import zipfile\n",
    "# !wget -q \"BACKUP_URL_HERE\" -O road_signs.zip\n",
    "# with zipfile.ZipFile('road_signs.zip', 'r') as z:\n",
    "#     z.extractall('road_signs_dataset')\n",
    "# DATASET_DIR = 'road_signs_dataset'\n",
    "# print(f\"‚úÖ Fallback dataset extracted to: {DATASET_DIR}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHfhlczqff_b"
   },
   "source": [
    "## Part B: Explore Dataset Structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fpNGCbO5ff_b",
    "outputId": "75ce1af5-aef8-473c-ae52-8b6dc90e8b54"
   },
   "source": [
    "# ============================================\n",
    "# EXPLORE DATASET STRUCTURE\n",
    "# ============================================\n",
    "\n",
    "# Show directory tree\n",
    "print(\"üìÇ Dataset Structure:\")\n",
    "print(\"=\" * 50)\n",
    "for root, dirs, files in os.walk(DATASET_DIR):\n",
    "    level = root.replace(DATASET_DIR, '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    folder_name = os.path.basename(root)\n",
    "    if level <= 2:\n",
    "        n_files = len(files)\n",
    "        print(f\"{indent}üìÅ {folder_name}/ ({n_files} files)\")\n",
    "\n",
    "# Count images per split\n",
    "print(\"\\nüìä Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    img_dir = os.path.join(DATASET_DIR, split, 'images')\n",
    "    if os.path.exists(img_dir):\n",
    "        n_images = len(glob.glob(os.path.join(img_dir, '*')))\n",
    "        print(f\"  {split:>8}: {n_images} images\")\n",
    "\n",
    "# Read and display data.yaml\n",
    "yaml_path = os.path.join(DATASET_DIR, 'data.yaml')\n",
    "with open(yaml_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"\\nüìã data.yaml Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in data_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SC-Pwv2ff_b"
   },
   "source": [
    "## Part C: Understand YOLO Annotation Format\n",
    "\n",
    "```\n",
    "class_id  x_center  y_center  width  height\n",
    "```\n",
    "\n",
    "All coordinates are **normalized** (0 to 1). Draw this on the whiteboard:\n",
    "```\n",
    "(0,0) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (1,0)\n",
    "  |                   |\n",
    "  |    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      |\n",
    "  |    ‚îÇ(cx,cy)‚îÇ      |\n",
    "  |    ‚îÇ   ¬∑   ‚îÇ      |\n",
    "  |    ‚îî‚îÄ‚îÄ‚îÄw‚îÄ‚îÄ‚îÄ‚îò      |\n",
    "  |        h          |\n",
    "(0,1) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (1,1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Zry2WnIdff_b",
    "outputId": "944c2f71-d1da-4a8b-a363-7c2d36a39c74"
   },
   "source": [
    "# ============================================\n",
    "# EXPLORE YOLO ANNOTATION FORMAT\n",
    "# ============================================\n",
    "\n",
    "# Get class names from data.yaml\n",
    "class_names = data_config['names']\n",
    "print(f\"üè∑Ô∏è Classes in this dataset:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "# Pick sample images from training set\n",
    "train_img_dir = os.path.join(DATASET_DIR, 'train', 'images')\n",
    "train_lbl_dir = os.path.join(DATASET_DIR, 'train', 'labels')\n",
    "sample_images = sorted(glob.glob(os.path.join(train_img_dir, '*')))[:5]\n",
    "\n",
    "# Show raw annotation file contents\n",
    "for img_path in sample_images[:2]:\n",
    "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    label_path = os.path.join(train_lbl_dir, img_name + '.txt')\n",
    "\n",
    "    print(f\"\\nüìÑ Image: {os.path.basename(img_path)}\")\n",
    "    print(f\"üìÑ Label: {os.path.basename(label_path)}\")\n",
    "    print(\"‚îÄ\" * 65)\n",
    "\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        print(f\"  Objects in this image: {len(lines)}\")\n",
    "        print(f\"  {'class_id':<10} {'x_center':<10} {'y_center':<10} \"\n",
    "              f\"{'width':<10} {'height':<10} {'class_name'}\")\n",
    "        print(f\"  {'‚îÄ' * 60}\")\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            cls_id = int(parts[0])\n",
    "            x_c, y_c, w, h = [float(p) for p in parts[1:]]\n",
    "            name = class_names[cls_id] if cls_id < len(class_names) else 'unknown'\n",
    "            print(f\"  {cls_id:<10} {x_c:<10.4f} {y_c:<10.4f} \"\n",
    "                  f\"{w:<10.4f} {h:<10.4f} {name}\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è No label file found\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Bx616ZHff_b"
   },
   "source": [
    "## Part D: Visualize Annotations on Images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "X3wgGW4iff_b",
    "outputId": "7e8ddadc-ee75-43e4-a064-4bfa1765d47a"
   },
   "source": [
    "# ============================================\n",
    "# HELPER: Convert YOLO format to pixel coordinates\n",
    "# ============================================\n",
    "\n",
    "def yolo_to_pixel(x_center, y_center, width, height, img_w, img_h):\n",
    "    \"\"\"\n",
    "    Convert YOLO normalized coordinates to pixel coordinates.\n",
    "    YOLO: (x_center, y_center, width, height) ‚Äî all 0 to 1\n",
    "    Pixel: (x1, y1, x2, y2) ‚Äî top-left and bottom-right corners\n",
    "    \"\"\"\n",
    "    x1 = (x_center - width / 2) * img_w\n",
    "    y1 = (y_center - height / 2) * img_h\n",
    "    x2 = (x_center + width / 2) * img_w\n",
    "    y2 = (y_center + height / 2) * img_h\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def visualize_yolo_annotations(img_path, label_path, class_names, ax=None):\n",
    "    \"\"\"Draw YOLO bounding boxes on an image.\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    img_w, img_h = img.size\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "    ax.imshow(img)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, max(len(class_names), 1)))\n",
    "\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split()\n",
    "                cls_id = int(parts[0])\n",
    "                x_c, y_c, w, h = [float(p) for p in parts[1:]]\n",
    "                x1, y1, x2, y2 = yolo_to_pixel(x_c, y_c, w, h, img_w, img_h)\n",
    "\n",
    "                color = colors[cls_id % len(colors)]\n",
    "                rect = patches.Rectangle(\n",
    "                    (x1, y1), x2 - x1, y2 - y1,\n",
    "                    linewidth=2, edgecolor=color, facecolor='none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "                name = class_names[cls_id] if cls_id < len(class_names) else f'cls_{cls_id}'\n",
    "                ax.text(x1, y1 - 5, name, fontsize=10, fontweight='bold',\n",
    "                       color='white',\n",
    "                       bbox=dict(boxstyle='round,pad=0.2',\n",
    "                                 facecolor=color, alpha=0.8))\n",
    "    ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "print(\"‚úÖ Helper functions defined.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "id": "5c-e8uHhff_c",
    "outputId": "59e31348-7893-41b0-e359-2e024e2f8c15"
   },
   "source": [
    "# ============================================\n",
    "# VISUALIZE ANNOTATIONS ON SAMPLE IMAGES\n",
    "# ============================================\n",
    "\n",
    "# Find images that have annotations\n",
    "annotated_samples = []\n",
    "for img_path in sorted(glob.glob(os.path.join(train_img_dir, '*'))):\n",
    "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    label_path = os.path.join(train_lbl_dir, img_name + '.txt')\n",
    "    if os.path.exists(label_path) and os.path.getsize(label_path) > 0:\n",
    "        annotated_samples.append((img_path, label_path))\n",
    "    if len(annotated_samples) >= 6:\n",
    "        break\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "for idx, (img_path, label_path) in enumerate(annotated_samples):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    visualize_yolo_annotations(img_path, label_path, class_names, ax=ax)\n",
    "    ax.set_title(os.path.basename(img_path), fontsize=10)\n",
    "\n",
    "plt.suptitle('Road Sign Dataset ‚Äî Ground Truth Annotations', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv3ec06Jff_c"
   },
   "source": [
    "## Part E: Dataset Statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "id": "16Re3T6Iff_c",
    "outputId": "f8e10b3a-c9ce-4e41-a648-4e1c83b93c4e"
   },
   "source": [
    "# ============================================\n",
    "# DATASET STATISTICS\n",
    "# ============================================\n",
    "\n",
    "# Count objects per class across training set\n",
    "class_counter = {name: 0 for name in class_names}\n",
    "total_objects = 0\n",
    "objects_per_image = []\n",
    "\n",
    "for label_path in glob.glob(os.path.join(train_lbl_dir, '*.txt')):\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    objects_per_image.append(len(lines))\n",
    "    for line in lines:\n",
    "        cls_id = int(line.strip().split()[0])\n",
    "        if cls_id < len(class_names):\n",
    "            class_counter[class_names[cls_id]] += 1\n",
    "        total_objects += 1\n",
    "\n",
    "print(\"üìä Training Set Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Images with annotations: {len(objects_per_image)}\")\n",
    "print(f\"  Total objects: {total_objects}\")\n",
    "print(f\"  Avg objects/image: {np.mean(objects_per_image):.1f}\")\n",
    "print(f\"  Max objects in one image: {max(objects_per_image) if objects_per_image else 0}\")\n",
    "print(f\"\\nüè∑Ô∏è Objects per Class:\")\n",
    "for name, count in sorted(class_counter.items(), key=lambda x: -x[1]):\n",
    "    bar = '‚ñà' * (count // 5)\n",
    "    print(f\"  {name:<20} {count:>5}  {bar}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names_list = list(class_counter.keys())\n",
    "counts_list = list(class_counter.values())\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(names_list)))\n",
    "axes[0].barh(names_list, counts_list, color=colors)\n",
    "axes[0].set_xlabel('Number of Annotations')\n",
    "axes[0].set_title('Class Distribution (Training Set)')\n",
    "for i, (n, c) in enumerate(zip(names_list, counts_list)):\n",
    "    axes[0].text(c + 2, i, str(c), va='center', fontsize=10)\n",
    "\n",
    "axes[1].hist(objects_per_image,\n",
    "             bins=range(0, max(objects_per_image) + 2),\n",
    "             edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[1].set_xlabel('Objects per Image')\n",
    "axes[1].set_ylabel('Number of Images')\n",
    "axes[1].set_title('Objects per Image Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbJrjPQDff_c"
   },
   "source": [
    "‚úÖ Task 2 Analysis\n",
    "Answer these questions based on YOUR dataset exploration:\n",
    "\n",
    "Q1. What does \"normalized coordinates\" mean in YOLO format? Why is this useful?\n",
    "\n",
    "Normalized coordinates means to map the range to [0, 1] for the x,y,w,h values by dividing by the image width and height. It is more efficient for the model to use smaller, normalized numbers for calculations and there isn't a need to train on images of different resolutions.\n",
    "\n",
    "Q2. Look at the class distribution. Is the dataset balanced or imbalanced? How might this affect model performance?\n",
    "\n",
    "The dataset is imbalanced as some classes have fewer (30) images compared to the rest (70+)\n",
    "\n",
    "Q3. How many files does a YOLO dataset need for each image? What if an image has no objects?\n",
    "\n",
    "YOLO dataset needs a .txt file describing the type and location of objects in the image. If the image has no objects, it needs an empty .txt file.\n",
    "\n",
    "Q4. What is data.yaml and what key information does it contain?\n",
    "\n",
    "The data.yaml file tells the YOLO algorithm where to find the data and how to interpret the numbers in the label files. It contains:\n",
    "\n",
    "(names of classes in order):\n",
    "\n",
    "names: ['bus_stop', 'do_not_enter', 'do_not_stop', 'do_not_turn_l', 'do_not_turn_r', 'do_not_u_turn', 'enter_left_lane', 'green_light', 'left_right_lane', 'no_parking', 'parking', 'ped_crossing', 'ped_zebra_cross', 'railway_crossing', 'red_light', 'stop', 't_intersection_l', 'traffic_light', 'u_turn', 'warning', 'yellow_light']\n",
    "\n",
    "(number of classes)\n",
    "\n",
    "nc: 21\n",
    "\n",
    "(versioning information)\n",
    "\n",
    "roboflow: {'license': 'CC BY 4.0', 'project': 'road-signs-6ih4y', 'url': 'https://universe.roboflow.com/roboflow-100/road-signs-6ih4y/dataset/2', 'version': 2, 'workspace': 'roboflow-100'}\n",
    "\n",
    "(file paths)\n",
    "\n",
    "test: ../test/images train: ../train/images val: ../valid/images\n",
    "\n",
    "Call instructor to verify completion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNEjkGgWff_c"
   },
   "source": [
    "---\n",
    "\n",
    "# Task 3: Fine-tune YOLO26 on Custom Data\n",
    "\n",
    "**Key points:**\n",
    "1. Start from COCO pre-trained weights (transfer learning!)\n",
    "2. Train for 10 epochs in class (more for mini project)\n",
    "3. IoU = Intersection over Union ‚Äî how well boxes match ground truth\n",
    "4. mAP = Mean Average Precision ‚Äî the standard detection metric\n",
    "\n",
    "**‚ö†Ô∏è TIME:** Training takes ~2‚Äì5 min for 10 epochs on T4 GPU. Reduce to 5 epochs if behind.\n",
    "\n",
    "**Draw IoU on the whiteboard:**\n",
    "```\n",
    "IoU = Area of Overlap / Area of Union\n",
    "\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ Predicted ‚îÇ\n",
    "   ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ    ‚îÇINTER ‚îÇ      ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n",
    "        ‚îÇ Ground Truth ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "IoU = 0.0 ‚Üí no overlap (terrible)\n",
    "IoU = 0.5 ‚Üí partial overlap (threshold for \"correct\")\n",
    "IoU = 1.0 ‚Üí perfect overlap (ideal)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Part A: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "UP6rekadff_c",
    "outputId": "8dcaa1bf-4c2c-4ae0-d0f4-92b790042f36"
   },
   "source": [
    "# ============================================\n",
    "# FINE-TUNE YOLO26 ON ROAD SIGNS\n",
    "# ============================================\n",
    "\n",
    "# but for detection. Key parameters to explain:\n",
    "# - epochs: passes through training data\n",
    "# - imgsz: YOLO resizes all images to this\n",
    "# - batch: adjust if GPU memory errors (try 8)\n",
    "# - patience: early stopping\n",
    "\n",
    "model = YOLO('yolo26n.pt')  # Fresh pre-trained model\n",
    "\n",
    "results = model.train(\n",
    "    data=os.path.join(DATASET_DIR, 'data.yaml'),\n",
    "    epochs=20,        # Low for class; use 50+ for real projects\n",
    "    imgsz=640,        # Standard YOLO input size\n",
    "    batch=16,         # Reduce to 8 if memory error\n",
    "    patience=5,       # Early stopping\n",
    "    lr0=0.005,         # Initial learning rate\n",
    "    verbose=True,\n",
    "    project='runs',\n",
    "    name='road_signs',\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "# Capture the actual save directory for use in later cells\n",
    "SAVE_DIR = str(results.save_dir)\n",
    "print(f\"\\n‚úÖ Training complete! Results saved to: {SAVE_DIR}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFyGaVflff_c"
   },
   "source": [
    "## Part B: Visualize Training Progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1073
    },
    "id": "pOY_apxvff_c",
    "outputId": "fe35cfea-1dd7-4750-8b40-3e878d27ca1c"
   },
   "source": [
    "# ============================================\n",
    "# VISUALIZE TRAINING CURVES\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results_csv = os.path.join(SAVE_DIR, 'results.csv')\n",
    "if os.path.exists(results_csv):\n",
    "    df = pd.read_csv(results_csv)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # Box loss\n",
    "    axes[0, 0].plot(df['epoch'], df['train/box_loss'],\n",
    "                     label='Train', color='blue')\n",
    "    axes[0, 0].plot(df['epoch'], df['val/box_loss'],\n",
    "                     label='Validation', color='red')\n",
    "    axes[0, 0].set_title('Box Loss (Lower is Better)')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Classification loss\n",
    "    axes[0, 1].plot(df['epoch'], df['train/cls_loss'],\n",
    "                     label='Train', color='blue')\n",
    "    axes[0, 1].plot(df['epoch'], df['val/cls_loss'],\n",
    "                     label='Validation', color='red')\n",
    "    axes[0, 1].set_title('Classification Loss (Lower is Better)')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # mAP50\n",
    "    axes[1, 0].plot(df['epoch'], df['metrics/mAP50(B)'],\n",
    "                     color='green', linewidth=2)\n",
    "    axes[1, 0].set_title('mAP@50 (Higher is Better)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # mAP50-95\n",
    "    axes[1, 1].plot(df['epoch'], df['metrics/mAP50-95(B)'],\n",
    "                     color='purple', linewidth=2)\n",
    "    axes[1, 1].set_title('mAP@50-95 (Higher is Better, Stricter)')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('YOLO26 Training ‚Äî Road Sign Detection', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print final metrics\n",
    "    last = df.iloc[-1]\n",
    "    print(\"\\nüìä Final Metrics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"  mAP@50:    {last['metrics/mAP50(B)']:.4f}\")\n",
    "    print(f\"  mAP@50-95: {last['metrics/mAP50-95(B)']:.4f}\")\n",
    "    print(f\"  Box loss:  {last['val/box_loss']:.4f}\")\n",
    "    print(f\"  Cls loss:  {last['val/cls_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è results.csv not found.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls6EtCa2ff_c"
   },
   "source": [
    "## Part C: Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "BSa0QgOyff_d",
    "outputId": "c0b72451-755a-4bec-a250-afecd9d9dd55"
   },
   "source": [
    "# ============================================\n",
    "# EVALUATE ON VALIDATION SET\n",
    "# ============================================\n",
    "\n",
    "best_model = YOLO(os.path.join(SAVE_DIR, 'weights', 'best.pt'))\n",
    "\n",
    "val_results = best_model.val(\n",
    "    data=os.path.join(DATASET_DIR, 'data.yaml'),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Validation Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  mAP@50:    {val_results.box.map50:.4f}\")\n",
    "print(f\"  mAP@50-95: {val_results.box.map:.4f}\")\n",
    "print(f\"  Precision: {val_results.box.mp:.4f}\")\n",
    "print(f\"  Recall:    {val_results.box.mr:.4f}\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Per-Class mAP@50:\")\n",
    "print(f\"  {'Class':<20} {'mAP@50':<10}\")\n",
    "print(f\"  {'‚îÄ' * 30}\")\n",
    "for i, name in enumerate(class_names):\n",
    "    if i < len(val_results.box.ap50):\n",
    "        print(f\"  {name:<20} {val_results.box.ap50[i]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXiGz8hMff_d"
   },
   "source": [
    "## Part D: Predictions on Validation Images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "id": "Q2i57Gtiff_d",
    "outputId": "94aafa25-4d55-4198-bdea-6b86fc399aa2"
   },
   "source": [
    "# ============================================\n",
    "# PREDICTIONS ON VALIDATION IMAGES\n",
    "# ============================================\n",
    "\n",
    "val_img_dir = os.path.join(DATASET_DIR, 'valid', 'images')\n",
    "val_images = sorted(glob.glob(os.path.join(val_img_dir, '*')))[:6]\n",
    "\n",
    "pred_results = best_model(val_images, conf=0.25, verbose=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "for idx, (img_path, result) in enumerate(zip(val_images, pred_results)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    annotated = result.plot()\n",
    "    ax.imshow(annotated[..., ::-1])\n",
    "    ax.set_title(f\"{os.path.basename(img_path)} \"\n",
    "                 f\"({len(result.boxes)} detections)\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Fine-tuned YOLO26 ‚Äî Predictions on Validation Set',\n",
    "             fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "9rFwYZQujGyu",
    "outputId": "68d66587-ce3a-464c-b1d7-f775dfcdd468"
   },
   "source": [
    "# visualize predictions and bounding boxes from the model on given classes\n",
    "\n",
    "import math\n",
    "# Define the class IDs you want to filter for\n",
    "# Based on your previous chart:\n",
    "# Run this to see the mapping\n",
    "print(best_model.names)\n",
    "\n",
    "# 1. Map your desired names to IDs automatically\n",
    "# This prevents errors if the class indices change\n",
    "target_names = ['red_light', 'yellow_light', 'railway_crossing', 't_intersection_l'] # Change these to the signs you want\n",
    "name_to_id = {v: k for k, v in best_model.names.items()}\n",
    "desired_class_ids = [name_to_id[name] for name in target_names if name in name_to_id]\n",
    "\n",
    "print(f\"Filtering for IDs: {desired_class_ids} ({target_names})\")\n",
    "\n",
    "# 2. Run inference on your validation folder\n",
    "# We use 'best_model' (your fine-tuned one) and the actual path variable 'val_img_dir'\n",
    "results = best_model.predict(\n",
    "    source=val_img_dir,\n",
    "    classes=desired_class_ids,\n",
    "    conf=0.5,\n",
    "    # exist_ok=True\n",
    ")\n",
    "\n",
    "filtered_results = []\n",
    "for r in results:\n",
    "    detected_ids = r.boxes.cls.cpu().numpy().astype(int)\n",
    "    if any(cls_id in desired_class_ids for cls_id in detected_ids):\n",
    "        filtered_results.append(r)\n",
    "\n",
    "# 4. Set up the 3-row grid display\n",
    "num_images = len(filtered_results[:30]) # Limit to first 30 matches for the grid\n",
    "cols = math.ceil(num_images / 3)\n",
    "rows = 3\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(30, 15))\n",
    "axes = axes.flatten() # Flatten to iterate easily\n",
    "\n",
    "for i in range(num_images):\n",
    "    res = filtered_results[i]\n",
    "    ann_img = res.plot() # Annotate image with boxes\n",
    "\n",
    "    # Convert BGR to RGB and show\n",
    "    axes[i].imshow(ann_img[:, :, ::-1])\n",
    "    axes[i].set_title(f\"Image: {res.path.split('/')[-1]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Hide any empty subplots if num_images is odd\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T-jEo4tGxd8m"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6Ge4ePSff_d"
   },
   "source": [
    "## Part E: Compare Pre-trained vs. Fine-tuned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3608
    },
    "id": "GynyupGZff_e",
    "outputId": "76e7cc53-98d1-4d3a-b1cb-23a24e1a63f1"
   },
   "source": [
    "# ============================================\n",
    "# COMPARE: PRE-TRAINED COCO vs. FINE-TUNED\n",
    "# ============================================\n",
    "\n",
    "compare_images = val_images[:15]\n",
    "\n",
    "fig, axes = plt.subplots(len(compare_images), 2,\n",
    "                          figsize=(16, 6 * len(compare_images)))\n",
    "\n",
    "for idx, img_path in enumerate(compare_images):\n",
    "    # Pre-trained COCO\n",
    "    res_coco = model_pretrained(img_path, conf=0.25, verbose=False)\n",
    "    axes[idx, 0].imshow(res_coco[0].plot()[..., ::-1])\n",
    "    axes[idx, 0].set_title(\n",
    "        f\"Pre-trained COCO ({len(res_coco[0].boxes)} det.)\", fontsize=12)\n",
    "    axes[idx, 0].axis('off')\n",
    "\n",
    "    # Fine-tuned\n",
    "    res_ft = best_model(img_path, conf=0.25, verbose=False)\n",
    "    axes[idx, 1].imshow(res_ft[0].plot()[..., ::-1])\n",
    "    axes[idx, 1].set_title(\n",
    "        f\"Fine-tuned Road Signs ({len(res_ft[0].boxes)} det.)\", fontsize=12)\n",
    "    axes[idx, 1].axis('off')\n",
    "\n",
    "plt.suptitle('Pre-trained COCO vs. Fine-tuned Road Sign Model', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° The COCO model knows generic 'stop sign' but not specific\")\n",
    "print(\"   road sign types. Fine-tuning teaches YOUR specific classes!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVPpvIQlff_e"
   },
   "source": [
    "‚úÖ Task 3 Analysis\n",
    "Answer these questions based on YOUR training results:\n",
    "\n",
    "Q1. What is your model's mAP@50? What would mAP=1.0 mean?\n",
    "\n",
    "mAP%50 is 0.7429 and the IoU >= 50%; The mAP=1.0 means perfect object detection which implies that every single object in the dataset was detected, its bounding box perfectly matched the ground truth (IoU = 1.0), and it was classified 100% confidence with no false positives or false negatives.\n",
    "\n",
    "Q2. Which road sign class does the model detect best? Which is hardest? Why?\n",
    "\n",
    "Best Class: t_intersection-l with an incredible mAP@50 of 0.956 (95.6%), closely followed by warning (0.929).\n",
    "\n",
    "Hardest Cass: red_light (0.365) and yellow_light (0.367) are the hardest. no_parking (0.433) also struggled significantly.\n",
    "\n",
    "Why: Traffic lights are notoriously difficult for object detection. They are physically very small in the frame, often blend into complex backgrounds (like trees or city lights), and the model has to distinguish which tiny colored circle is illuminated. no_parking signs are likely difficult because they rely on reading fine text and smaller symbols, which get easily blurred at lower resolutions or greater distances.\n",
    "\n",
    "Q3. Name 2 similarities and 2 differences between fine-tuning for detection vs. classification (Week 7).\n",
    "\n",
    "Similarities:\n",
    "\n",
    "Transfer learning base - both start with a model pre-trained on a massive dataset and update its weights to learn the specific features of our custom dataset.\n",
    "training mechanices - both utilize a training loop with similar hyperparameters and evaluate performance on a validation set to monitor for overfitting.\n",
    "Differences:\n",
    "\n",
    "annotation/data format - image classification just needs one lable per image while object detection requires complex annotations for every image, detailing the exact spatial coordinates of every single object present.\n",
    "model ouput & loss function - a classification model ouputs a single probability score for the shole image and calculates simple classification loss, while a detecion model outpus mltiple bounding boxes and class probabilities simultaneously, requiring a complex, multi-part loss function that calculates both localization loss and classification loss.\n",
    "Q4. Looking at training curves, would more epochs help? What signs indicate overfitting?\n",
    "\n",
    "The validation curves for box loss stays relatively the same but the training loss goes down which means it is a sign of overfitting. More epochs most likely won't help in this case. The classification loss curves also plateau so more epoches will also not help.\n",
    "\n",
    "Call instructor to verify completion!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2cec99b"
   },
   "source": [
    "# Task\n",
    "Please provide a list of target road sign class names that you want to visualize. For example, `['railway_crossing', 'ped_crossing']`.\n",
    "\n",
    "Here are the available class names from the dataset: `['bus_stop', 'do_not_enter', 'do_not_stop', 'do_not_turn_l', 'do_not_turn_r', 'do_not_u_turn', 'enter_left_lane', 'green_light', 'left_right_lane', 'no_parking', 'parking', 'ped_crossing', 'ped_zebra_cross', 'railway_crossing', 'red_light', 'stop', 't_intersection_l', 'traffic_light', 'u_turn', 'warning', 'yellow_light']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6075e6a3"
   },
   "source": [
    "## Define Target Road Signs\n",
    "\n",
    "### Subtask:\n",
    "Define a list of specific road sign class names and their corresponding class IDs that you want to visualize predictions for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cafb6b1"
   },
   "source": [
    "**Reasoning**:\n",
    "The subtask requires defining target road sign names and their corresponding class IDs for visualization. I will create a Python code block to define these lists, mapping the names to IDs using the previously defined `class_names`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8697ec5c",
    "outputId": "23767ae5-ea1a-47b5-a0a8-33028292bf99"
   },
   "source": [
    "TARGET_ROAD_SIGNS = ['railway_crossing', 'ped_crossing', 'stop', 'traffic_light']\n",
    "\n",
    "TARGET_CLASS_IDS = [class_names.index(name) for name in TARGET_ROAD_SIGNS if name in class_names]\n",
    "\n",
    "print(f\"Target Road Signs: {TARGET_ROAD_SIGNS}\")\n",
    "print(f\"Corresponding Class IDs: {TARGET_CLASS_IDS}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a1658c7"
   },
   "source": [
    "## Select Validation Images for Prediction\n",
    "\n",
    "### Subtask:\n",
    "Choose a set of validation images from the dataset to run inference on. These will be the images where we look for predictions of the target road signs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82005068"
   },
   "source": [
    "**Reasoning**:\n",
    "First, I'll define the directory containing the validation images. Then, I'll use `glob.glob` to retrieve all image paths within this directory and sort them to ensure consistent selection. Finally, I'll select a subset of these images for prediction and visualization as per the instructions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "f4f1262d",
    "outputId": "abcf2a40-9369-41c0-8f2a-3d6fa53b8d78"
   },
   "source": [
    "val_img_dir = os.path.join(DATASET_DIR, 'valid', 'images')\n",
    "\n",
    "# Get all image paths in the validation directory\n",
    "all_val_images = sorted(glob.glob(os.path.join(val_img_dir, '*')))\n",
    "\n",
    "# Select a subset of images (e.g., the first 9) for prediction\n",
    "selected_val_images = all_val_images[:9]\n",
    "\n",
    "print(f\"Selected {len(selected_val_images)} validation images for prediction:\")\n",
    "for img_path in selected_val_images:\n",
    "    print(f\"- {os.path.basename(img_path)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3b660fc"
   },
   "source": [
    "**Reasoning**:\n",
    "Now that the validation images are selected and target class IDs are defined, I will use the fine-tuned model (`best_model`) to make predictions on these images. I will filter the predictions to only show the `TARGET_CLASS_IDS` and set a confidence threshold for clearer visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1355
    },
    "id": "8e452760",
    "outputId": "746bc440-e683-43f8-d792-0ae4afd78a7a"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run predictions on the selected validation images, filtering by target class IDs\n",
    "# Use a reasonable confidence threshold, e.g., 0.5\n",
    "pred_results_filtered = best_model(selected_val_images, conf=0.5, classes=TARGET_CLASS_IDS, verbose=False)\n",
    "\n",
    "# Visualize the filtered predictions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 18))\n",
    "axes = axes.flatten() # Flatten the 2x3 array of axes for easy iteration\n",
    "\n",
    "for idx, (img_path, result) in enumerate(zip(selected_val_images, pred_results_filtered)):\n",
    "    ax = axes[idx]\n",
    "    annotated = result.plot() # Ultralytics .plot() method draws boxes and labels\n",
    "    ax.imshow(annotated[..., ::-1]) # Convert BGR to RGB for matplotlib\n",
    "    ax.set_title(f\"Predictions: {os.path.basename(img_path)} ({len(result.boxes)} detections)\", fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Fine-tuned YOLO26 - Filtered Predictions on Validation Set', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Displayed predictions for target classes: {TARGET_ROAD_SIGNS}\")\n",
    "print(\"Confidence threshold for display: 0.5\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56af1b1c"
   },
   "source": [
    "## Summarize Detected Objects\n",
    "\n",
    "### Subtask:\n",
    "Provide a summary of the number of target road signs detected by the fine-tuned model across the visualized images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58c9f846"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Q&A\n",
    "The target road sign class names selected for visualization are 'railway\\_crossing', 'ped\\_crossing', 'stop', and 'traffic\\_light'.\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "*   **Target Road Signs and IDs:** The target road signs for visualization were set to \\['railway\\_crossing', 'ped\\_crossing', 'stop', 'traffic\\_light'\\], corresponding to class IDs \\[13, 11, 15, 17\\].\n",
    "*   **Validation Image Selection:** Nine validation images were selected from the `valid/images` directory to perform inference.\n",
    "*   **Filtered Predictions:** Predictions were successfully run on the selected validation images, focusing only on the defined target road signs with a confidence threshold of 0.5.\n",
    "*   **Prediction Visualization:** The filtered predictions were visualized, displaying annotated images with bounding boxes for the detected target road signs.\n",
    "\n",
    "### Insights or Next Steps\n",
    "*   The process successfully demonstrated the fine-tuned model's capability to identify and visualize specific road signs, indicating a functional object detection pipeline for the chosen classes.\n",
    "*   To gain a deeper understanding of model performance, the next step should involve quantifying the total number of detections for each target road sign across all analyzed images.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "8b87bVVHxil8",
    "outputId": "b62ba65b-d546-4aad-d502-5f196427e604"
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "import math\n",
    "import random  # Import random for sampling\n",
    "\n",
    "# 1. Load your fine-tuned model weights\n",
    "model = best_model\n",
    "\n",
    "# 2. Get dataset configuration\n",
    "with open(os.path.join(DATASET_DIR, 'data.yaml'), 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "class_names = data_config['names']\n",
    "\n",
    "val_img_dir = os.path.join(DATASET_DIR, 'valid', 'images')\n",
    "val_lbl_dir = os.path.join(DATASET_DIR, 'valid', 'labels')\n",
    "\n",
    "# 3. Analyze ALL images for False Negatives\n",
    "missed_samples = []\n",
    "image_paths = sorted(glob.glob(os.path.join(val_img_dir, '*')))\n",
    "\n",
    "for img_path in image_paths:\n",
    "    img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "    label_path = os.path.join(val_lbl_dir, img_name + '.txt')\n",
    "\n",
    "    if not os.path.exists(label_path):\n",
    "        continue\n",
    "\n",
    "    with open(label_path, 'r') as f:\n",
    "        gt_classes = set(int(line.split()[0]) for line in f.readlines())\n",
    "\n",
    "    results = model.predict(img_path, conf=0.5, verbose=False)\n",
    "    pred_classes = set(results[0].boxes.cls.cpu().numpy().astype(int))\n",
    "\n",
    "    missed_ids = gt_classes - pred_classes\n",
    "\n",
    "    if missed_ids:\n",
    "        missed_names = [class_names[cid] for cid in missed_ids]\n",
    "        missed_samples.append((results[0], missed_names))\n",
    "\n",
    "# 4. RANDOM SAMPLING\n",
    "num_to_display = 10\n",
    "if len(missed_samples) > 0:\n",
    "    # Randomly pick images from the list of failures\n",
    "    # If we found fewer failures than num_to_display, take all of them\n",
    "    current_sample_size = min(len(missed_samples), num_to_display)\n",
    "    random_missed_samples = random.sample(missed_samples, current_sample_size)\n",
    "\n",
    "    # 5. Set up the 2-row grid display\n",
    "    cols = math.ceil(current_sample_size / 2)\n",
    "    rows = 2\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(28, 14))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(current_sample_size):\n",
    "        res, missed_list = random_missed_samples[i]\n",
    "        ann_img = res.plot()\n",
    "\n",
    "        axes[i].imshow(ann_img[..., ::-1])\n",
    "        axes[i].set_title(f\"FILE: {os.path.basename(res.path)}\\nMISSED: {', '.join(missed_list)}\",\n",
    "                          color='red', fontsize=12, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Zero False Negatives found!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
