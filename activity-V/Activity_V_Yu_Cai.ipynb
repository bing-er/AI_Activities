{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv3dcBRgXwsy"
   },
   "source": [
    "# Week 6: Computer Vision I ‚Äî CNN Fundamentals\n",
    "## COMP 9130 ‚Äî Applied Artificial Intelligence\n",
    "\n",
    "---\n",
    "\n",
    "**Today's Dataset:** Wildfire Detection Image Data (~1,900 images, binary: fire vs no-fire)\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Build a CNN from scratch using Conv2D and pooling layers\n",
    "2. Train a CNN for binary image classification (wildfire detection)\n",
    "3. Use Keras augmentation layers to reduce overfitting\n",
    "4. Visualize CNN internals ‚Äî feature maps and learned filters\n",
    "\n",
    "**Today's Tasks:**\n",
    "| Time | Task | Points |\n",
    "|------|------|--------|\n",
    "| 0:00‚Äì0:20 | Quiz 5 | 5 pts |\n",
    "| 0:30‚Äì1:20 | Task 1: Build & Train a Basic CNN | 5 pts |\n",
    "| 1:30‚Äì2:20 | Task 2: Data Augmentation | 5 pts |\n",
    "| 2:20‚Äì2:55 | Task 3: Visualize CNN Internals | 5 pts |\n",
    "\n",
    "**Pair Programming Reminder:** One person types, one person thinks. Switch roles between tasks!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZK5iEKPqXwsz"
   },
   "source": [
    "## Setup & Imports\n",
    "\n",
    "Run this cell first. Make sure you have GPU enabled: **Runtime ‚Üí Change runtime type ‚Üí T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vj7MszihXws0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SETUP ‚Äî Run this first!\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Verify versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version:      {np.__version__}\")\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\n‚úÖ GPU available: {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKji3dl3Xws0"
   },
   "source": [
    "## Download & Load the Wildfire Dataset\n",
    "\n",
    "We're using a real wildfire detection dataset from Kaggle: ~1,900 images of fire/no-fire scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zthq9ba0Xws0"
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!pip install -q kagglehub\n",
    "import kagglehub\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"brsdincer/wildfire-detection-image-data\")\n",
    "print(f\"Dataset downloaded to: {dataset_path}\")\n",
    "\n",
    "# Print structure to understand folder layout\n",
    "print(\"\\nDataset structure:\")\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    level = root.replace(dataset_path, '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/  ({len(files)} files)\")\n",
    "    if level >= 3:\n",
    "        continue\n",
    "\n",
    "# Find the directory with exactly 2 class subfolders containing images\n",
    "data_dir = None\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    if len(dirs) == 2:\n",
    "        has_images = False\n",
    "        for d in dirs:\n",
    "            subpath = os.path.join(root, d)\n",
    "            img_files = [f for f in os.listdir(subpath)\n",
    "                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            if len(img_files) > 10:\n",
    "                has_images = True\n",
    "                break\n",
    "        if has_images:\n",
    "            data_dir = root\n",
    "            break\n",
    "\n",
    "if data_dir is None:\n",
    "    raise FileNotFoundError(\"Could not find class folders. Check the structure above.\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data directory: {data_dir}\")\n",
    "for d in sorted(os.listdir(data_dir)):\n",
    "    sub = os.path.join(data_dir, d)\n",
    "    if os.path.isdir(sub):\n",
    "        n = len([f for f in os.listdir(sub) if f.lower().endswith(('.png','.jpg','.jpeg'))])\n",
    "        print(f\"   - {d}: {n} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1are9_BXws0"
   },
   "outputs": [],
   "source": [
    "# Load images into TensorFlow datasets\n",
    "IMG_SIZE = 250  # Original resolution (250x250)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=42,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=42,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary'\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QColsXLgXws1"
   },
   "source": [
    "## Explore the Data\n",
    "\n",
    "Let's see what our images look like and understand their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubLRxIaRXws1"
   },
   "outputs": [],
   "source": [
    "# Count images per class\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(data_dir, class_name)\n",
    "    n_images = len([f for f in os.listdir(class_path)\n",
    "                    if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    print(f\"  {class_name}: {n_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9WOlCucXws1"
   },
   "outputs": [],
   "source": [
    "# Visualize sample images from both classes\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Keras assigns classes alphabetically: fire=0, nofire=1\n",
    "fire_class = class_names.index('fire') if 'fire' in class_names else 0\n",
    "nofire_class = 1 - fire_class\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    fire_mask = labels.numpy().flatten() == fire_class\n",
    "    nofire_mask = labels.numpy().flatten() == nofire_class\n",
    "\n",
    "    # Show fire images (top row)\n",
    "    fire_images = images[fire_mask]\n",
    "    for i in range(min(5, len(fire_images))):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(fire_images[i].numpy().astype('uint8'))\n",
    "        plt.title('Fire', fontsize=11, fontweight='bold', color='red')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Show no-fire images (bottom row)\n",
    "    nofire_images = images[nofire_mask]\n",
    "    for i in range(min(5, len(nofire_images))):\n",
    "        plt.subplot(2, 5, i + 6)\n",
    "        plt.imshow(nofire_images[i].numpy().astype('uint8'))\n",
    "        plt.title('No Fire', fontsize=11, fontweight='bold', color='green')\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.suptitle('Wildfire Detection Dataset ‚Äî Sample Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Yya4F6AXws1"
   },
   "outputs": [],
   "source": [
    "# Inspect image properties\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(f\"Batch shape:  {images.shape}\")\n",
    "    print(f\"Label shape:  {labels.shape}\")\n",
    "    print(f\"Pixel range:  [{images.numpy().min():.0f}, {images.numpy().max():.0f}]\")\n",
    "    print(f\"\\nSingle image shape: {images[0].shape}\")\n",
    "    print(f\"  ‚Üí Height: {images[0].shape[0]}, Width: {images[0].shape[1]}, Channels: {images[0].shape[2]}\")\n",
    "\n",
    "# Why CNNs?\n",
    "print(f\"\\nüí° If we flattened this image: {250*250*3:,} input features!\")\n",
    "print(f\"   Compare to MNIST flattened: {28*28} input features\")\n",
    "print(f\"   ‚Üí This is why CNNs use weight sharing through convolutions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwTi_wdcXws1"
   },
   "outputs": [],
   "source": [
    "# Optimize data pipeline\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(\"‚úÖ Data pipeline optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ry-eKHVXws1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROVIDED: Helper function for plotting training curves\n",
    "# (You'll use this in Tasks 1 and 2)\n",
    "# ============================================================\n",
    "\n",
    "def plot_training_curves(history, title='Model Training'):\n",
    "    \"\"\"Plot accuracy and loss curves for training and validation.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    ax1.plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    ax1.set_title(f'{title} ‚Äî Accuracy', fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim([0.4, 1.05])\n",
    "\n",
    "    ax2.plot(history.history['loss'], label='Train', linewidth=2)\n",
    "    ax2.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    ax2.set_title(f'{title} ‚Äî Loss', fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    gap = final_train_acc - final_val_acc\n",
    "    print(f\"\\nüìä Final Results:\")\n",
    "    print(f\"   Train Accuracy:      {final_train_acc:.4f} ({final_train_acc*100:.1f}%)\")\n",
    "    print(f\"   Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.1f}%)\")\n",
    "    print(f\"   Gap (overfit check): {gap:.4f} ({gap*100:.1f}%)\")\n",
    "    if gap > 0.15:\n",
    "        print(f\"   ‚ö†Ô∏è  Large gap ‚Äî model is OVERFITTING\")\n",
    "    elif gap > 0.05:\n",
    "        print(f\"   ‚ö° Moderate gap ‚Äî some overfitting\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Small gap ‚Äî good generalization!\")\n",
    "\n",
    "\n",
    "def show_predictions(model, dataset, class_names, n=10):\n",
    "    \"\"\"Show model predictions on sample images.\"\"\"\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for images, labels in dataset.take(1):\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        for i in range(min(n, len(images))):\n",
    "            plt.subplot(2, 5, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype('uint8'))\n",
    "            pred_prob = predictions[i][0]\n",
    "            pred_label = 1 if pred_prob > 0.5 else 0\n",
    "            true_label = int(labels[i].numpy())\n",
    "            color = 'green' if pred_label == true_label else 'red'\n",
    "            plt.title(f'Pred: {class_names[pred_label]}\\n({pred_prob:.2f})\\nTrue: {class_names[true_label]}',\n",
    "                     fontsize=9, color=color, fontweight='bold')\n",
    "            plt.axis('off')\n",
    "    plt.suptitle('Predictions (Green=Correct, Red=Wrong)', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfJ8GDhPXws1"
   },
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TASK 1: Build & Train a Basic CNN (50 min)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "Build your first Convolutional Neural Network for wildfire detection!\n",
    "\n",
    "**Architecture to build:**\n",
    "```\n",
    "Input (250√ó250√ó3) ‚Üí Rescaling (0-1)\n",
    "  ‚Üí Conv2D(32, 3√ó3, relu) ‚Üí MaxPool(2√ó2)\n",
    "  ‚Üí Conv2D(64, 3√ó3, relu) ‚Üí MaxPool(2√ó2)\n",
    "  ‚Üí Conv2D(128, 3√ó3, relu) ‚Üí MaxPool(2√ó2)\n",
    "  ‚Üí Flatten ‚Üí Dense(128, relu) ‚Üí Dense(1, sigmoid)\n",
    "```\n",
    "\n",
    "**Key concepts:**\n",
    "- `Rescaling(1./255)`: Normalizes pixels from [0, 255] to [0, 1]\n",
    "- `Conv2D(filters, kernel_size, activation)`: Applies learned filters to detect patterns\n",
    "- `MaxPooling2D(pool_size)`: Reduces spatial dimensions by keeping max values\n",
    "- `Flatten`: Converts 3D feature maps to 1D for the Dense layers\n",
    "- `Dense(1, sigmoid)`: Outputs probability of fire (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyIz3iCYXws1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Build your CNN model\n",
    "# ============================================================\n",
    "# Follow the architecture diagram above.\n",
    "# Hint: Use models.Sequential([...]) with the layers listed.\n",
    "\n",
    "model_basic = models.Sequential([\n",
    "    # TODO: Add Rescaling layer (normalize pixels to 0-1)\n",
    "    #       Don't forget input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "    # TODO: Conv Block 1 ‚Äî Conv2D(32 filters, 3x3, relu) + MaxPooling2D(2x2)\n",
    "\n",
    "    # TODO: Conv Block 2 ‚Äî Conv2D(64 filters, 3x3, relu) + MaxPooling2D(2x2)\n",
    "\n",
    "    # TODO: Conv Block 3 ‚Äî Conv2D(128 filters, 3x3, relu) + MaxPooling2D(2x2)\n",
    "\n",
    "    # TODO: Classification head ‚Äî Flatten, Dense(128, relu), Dense(1, sigmoid)\n",
    "\n",
    "])\n",
    "\n",
    "# Display the architecture\n",
    "model_basic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJdNkz7SXws1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Compile the model\n",
    "# ============================================================\n",
    "# Use:\n",
    "#   optimizer = 'adam'\n",
    "#   loss = 'binary_crossentropy' (because binary classification)\n",
    "#   metrics = ['accuracy']\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8LdsM7wXws1"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Train the model for 50 epochs\n",
    "# ============================================================\n",
    "# Use model_basic.fit() with:\n",
    "#   - train_ds as training data\n",
    "#   - val_ds as validation_data\n",
    "#   - epochs=15\n",
    "# Save the result to history_basic\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# history_basic = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykHEavjwXws1"
   },
   "outputs": [],
   "source": [
    "# Plot training curves (use the provided helper function)\n",
    "plot_training_curves(history_basic, title='Basic CNN (No Augmentation)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhRyv2nVXws2"
   },
   "outputs": [],
   "source": [
    "# Show predictions on validation images\n",
    "show_predictions(model_basic, val_ds, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN0YEgyvXws2"
   },
   "source": [
    "## ‚úÖ Task 1 Checkpoint ‚Äî Show your instructor\n",
    "\n",
    "| Criteria | Points |\n",
    "|----------|--------|\n",
    "| CNN built, trained, ‚â•75% val accuracy, analysis answered | 5 pts |\n",
    "| CNN trains but accuracy <75% or incomplete analysis | 3 pts |\n",
    "| Attempted but doesn't train properly | 1 pt |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpuk_ApPXws2"
   },
   "source": [
    "### üìù Task 1 Analysis Questions\n",
    "\n",
    "**Answer using YOUR specific results (numbers from YOUR training).**\n",
    "\n",
    "**Q1:** What is your model's final training accuracy and validation accuracy? Is the model overfitting? How can you tell from the training curves?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2:** Look at the `model.summary()` output. Which layer has the most parameters? Why does the Flatten ‚Üí Dense connection create so many parameters?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q3:** Why do we increase the number of filters (32 ‚Üí 64 ‚Üí 128) as we go deeper? What happens to the spatial dimensions at the same time?\n",
    "\n",
    "*Your answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXZPu1b3Xws2"
   },
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TASK 2: Data Augmentation (50 min)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "Your Task 1 model likely overfits (training accuracy >> validation accuracy). Let's fix that with **data augmentation** ‚Äî applying random transforms to training images so the model sees different versions each epoch.\n",
    "\n",
    "We'll use modern **Keras augmentation layers** that integrate directly into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2NS_5sLXws2"
   },
   "source": [
    "## Part 2A: Visualize What Augmentation Does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHbQ2zyiXws2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Define augmentation layers\n",
    "# ============================================================\n",
    "# These layers apply random transformations during training ONLY.\n",
    "# During prediction, they automatically deactivate.\n",
    "\n",
    "data_augmentation = models.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomContrast(0.2),\n",
    "], name='data_augmentation')\n",
    "\n",
    "print(\"Augmentation layers:\")\n",
    "for layer in data_augmentation.layers:\n",
    "    print(f\"  - {layer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVTyuBpGXws2"
   },
   "outputs": [],
   "source": [
    "# Visualize augmentation on a single image\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    sample_image = images[0]\n",
    "\n",
    "    plt.subplot(4, 4, 1)\n",
    "    plt.imshow(sample_image.numpy().astype('uint8'))\n",
    "    plt.title('ORIGINAL', fontweight='bold', fontsize=11)\n",
    "    plt.axis('off')\n",
    "\n",
    "    for i in range(15):\n",
    "        augmented = data_augmentation(tf.expand_dims(sample_image, 0), training=True)\n",
    "        plt.subplot(4, 4, i + 2)\n",
    "        plt.imshow(augmented[0].numpy().astype('uint8'))\n",
    "        plt.title(f'Augmented #{i+1}', fontsize=9)\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.suptitle('Data Augmentation ‚Äî Same Image, Different Transforms', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Each epoch, the model sees a DIFFERENT version of every training image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZIMbJgUXws2"
   },
   "source": [
    "## Part 2B: Build CNN with Augmentation + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywlzv8QlXws2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Build a CNN with augmentation layers built in\n",
    "# ============================================================\n",
    "# Same CNN architecture as Task 1, BUT:\n",
    "#   1. Add augmentation layers AFTER Rescaling (before Conv layers)\n",
    "#   2. Add Dropout(0.3) AFTER Flatten (before Dense)\n",
    "#   3. #\n",
    "# Architecture:\n",
    "#   Rescaling ‚Üí RandomFlip ‚Üí RandomRotation ‚Üí RandomZoom ‚Üí RandomContrast\n",
    "#   ‚Üí Conv2D(32) ‚Üí MaxPool ‚Üí Conv2D(64) ‚Üí MaxPool ‚Üí Conv2D(128) ‚Üí MaxPool\n",
    "#   ‚Üí Flatten ‚Üí Dropout(0.3) ‚Üí Dense(128, relu) ‚Üí Dense(1, sigmoid)\n",
    "\n",
    "model_augmented = models.Sequential([\n",
    "    # TODO: Rescaling layer with input_shape\n",
    "\n",
    "    # TODO: Add the 4 augmentation layers\n",
    "\n",
    "    # TODO: Same 3 Conv+Pool blocks as Task 1\n",
    "\n",
    "    # TODO: Flatten ‚Üí Dropout(0.3) ‚Üí Dense(128, relu) ‚Üí Dense(1, sigmoid)\n",
    "\n",
    "])\n",
    "\n",
    "model_augmented.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80WB-APkXws2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Compile and train the augmented model\n",
    "# ============================================================\n",
    "# Same compile settings as Task 1\n",
    "# Train for 50 epochs (same as basic) (same as basic for fair comparison)\n",
    "\n",
    "EPOCHS_AUG = 50\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# model_augmented.compile(...)\n",
    "# history_augmented = model_augmented.fit(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3lYN2hmXws2"
   },
   "source": [
    "## Part 2C: Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0iH1w9QXws2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Plot both models' training curves side by side\n",
    "# ============================================================\n",
    "# Create a figure with 2 subplots (1 row, 2 columns)\n",
    "# Left: Basic CNN accuracy curves (train + val)\n",
    "# Right: Augmented CNN accuracy curves (train + val)\n",
    "#\n",
    "# Hint: Use history_basic.history['accuracy'] and\n",
    "#       history_augmented.history['accuracy']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# YOUR CODE HERE ‚Äî plot train/val accuracy for both models\n",
    "\n",
    "\n",
    "plt.suptitle('Effect of Data Augmentation on Overfitting', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eVyt3YuXws2"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Create a comparison table\n",
    "# ============================================================\n",
    "# Fill in your actual numbers from training\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "basic_train = history_basic.history['accuracy'][-1]\n",
    "basic_val = history_basic.history['val_accuracy'][-1]\n",
    "aug_train = history_augmented.history['accuracy'][-1]\n",
    "aug_val = history_augmented.history['val_accuracy'][-1]\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Basic CNN', 'Augmented CNN'],\n",
    "    'Train Acc': [f'{basic_train:.4f}', f'{aug_train:.4f}'],\n",
    "    'Val Acc': [f'{basic_val:.4f}', f'{aug_val:.4f}'],\n",
    "    'Gap': [f'{basic_train - basic_val:.4f}', f'{aug_train - aug_val:.4f}']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyH1eILXXws2"
   },
   "outputs": [],
   "source": [
    "# Show predictions from augmented model\n",
    "show_predictions(model_augmented, val_ds, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH-mDj13Xws3"
   },
   "source": [
    "## ‚úÖ Task 2 Checkpoint ‚Äî Show your instructor\n",
    "\n",
    "| Criteria | Points |\n",
    "|----------|--------|\n",
    "| Augmented model trained, comparison table filled, analysis answered | 5 pts |\n",
    "| Model trains but incomplete comparison | 3 pts |\n",
    "| Attempted but augmentation not working | 1 pt |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpKQHADXXws3"
   },
   "source": [
    "### üìù Task 2 Analysis Questions\n",
    "\n",
    "**Answer using YOUR specific comparison numbers.**\n",
    "\n",
    "**Q1:** Compare the train-validation gap between your basic CNN and augmented CNN. Which model overfits more? Use specific numbers from your comparison table.\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2:** Why does the augmented model have LOWER training accuracy than the basic model? Is that a problem?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q3:** We added Dropout layers in addition to augmentation. Why both? What does each technique address?\n",
    "\n",
    "*Your answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZZ-yvBNXws3"
   },
   "source": [
    "---\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# TASK 3: Visualize CNN Internals (35 min)\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "Now let's look INSIDE the CNN to understand what it actually learned.\n",
    "\n",
    "We'll visualize:\n",
    "- **Feature maps** (activations): What the CNN \"sees\" at each layer\n",
    "- **Learned filters** (weights): The patterns the CNN learned to detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLw9B69_Xws3"
   },
   "source": [
    "## Part 3A: Visualize Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVQikFOfXws3"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Create a sub-model that outputs feature maps\n",
    "# ============================================================\n",
    "# We need to extract the outputs of each Conv2D layer.\n",
    "# In Keras 3.x, we rebuild the forward pass with a Functional API Input.\n",
    "#\n",
    "# Steps:\n",
    "#   1. Create keras.Input with shape (IMG_SIZE, IMG_SIZE, 3)\n",
    "#   2. Loop through model_basic.layers, passing x through each\n",
    "#   3. Collect outputs from Conv2D layers\n",
    "#   4. Build models.Model(inputs=inp, outputs=conv_outputs)\n",
    "\n",
    "inp = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "x = inp\n",
    "conv_outputs = []\n",
    "conv_names = []\n",
    "for layer in model_basic.layers:\n",
    "    x = layer(x)\n",
    "    if isinstance(layer, layers.Conv2D):\n",
    "        conv_outputs.append(x)\n",
    "        conv_names.append(layer.name)\n",
    "        print(f\"  {layer.name}: output shape = {x.shape}\")\n",
    "\n",
    "# TODO: Build the feature map model using models.Model()\n",
    "# YOUR CODE HERE\n",
    "# feature_map_model = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bl4m5nCiXws3"
   },
   "outputs": [],
   "source": [
    "# Select a fire image and a no-fire image for comparison\n",
    "# Keras assigns classes alphabetically: fire=0, nofire=1\n",
    "print(f\"Class mapping: {class_names}\")\n",
    "\n",
    "fire_class = class_names.index('fire') if 'fire' in class_names else 0\n",
    "nofire_class = 1 - fire_class\n",
    "\n",
    "for images, labels_batch in val_ds.take(1):\n",
    "    fire_idx = None\n",
    "    nofire_idx = None\n",
    "    for i in range(len(labels_batch)):\n",
    "        label = int(labels_batch[i].numpy())\n",
    "        if label == fire_class and fire_idx is None:\n",
    "            fire_idx = i\n",
    "        elif label == nofire_class and nofire_idx is None:\n",
    "            nofire_idx = i\n",
    "        if fire_idx is not None and nofire_idx is not None:\n",
    "            break\n",
    "\n",
    "    test_fire = images[fire_idx]\n",
    "    test_nofire = images[nofire_idx]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax1.imshow(test_fire.numpy().astype('uint8'))\n",
    "ax1.set_title('Test: Fire', fontweight='bold')\n",
    "ax1.axis('off')\n",
    "ax2.imshow(test_nofire.numpy().astype('uint8'))\n",
    "ax2.set_title('Test: No Fire', fontweight='bold')\n",
    "ax2.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJA_kPi3Xws4"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Get feature maps for the fire image and display them\n",
    "# ============================================================\n",
    "# Steps:\n",
    "#   1. Add batch dimension: tf.expand_dims(test_fire, 0)\n",
    "#   2. Pass through feature_map_model.predict()\n",
    "#   3. For each layer, show the first 16 feature maps in a 2x8 grid\n",
    "#\n",
    "# Hint: feature_maps[layer_idx] has shape (1, H, W, num_filters)\n",
    "#       Display feature_maps[layer_idx][0, :, :, filter_idx]\n",
    "\n",
    "test_input = tf.expand_dims(test_fire, 0)\n",
    "feature_maps = feature_map_model.predict(test_input, verbose=0)\n",
    "\n",
    "# conv_names was defined in the cell above\n",
    "\n",
    "# YOUR CODE HERE ‚Äî display feature maps for each layer\n",
    "# Use plt.subplot() and plt.imshow() with cmap='viridis'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn6n7A3nXws4"
   },
   "source": [
    "## Part 3B: Visualize Learned Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qPr28uUXws4"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Visualize learned filters from each Conv layer\n",
    "# ============================================================\n",
    "# Steps:\n",
    "#   1. Loop through Conv2D layers in model_basic\n",
    "#   2. Get weights with layer.get_weights() ‚Üí returns [filters, biases]\n",
    "#   3. Normalize filter values to [0, 1] for display\n",
    "#   4. Show first 16 filters in a 2x8 grid\n",
    "#\n",
    "# For the first layer (3 input channels), you can display filters as RGB\n",
    "# For deeper layers, show the mean across input channels\n",
    "\n",
    "for layer in model_basic.layers:\n",
    "    if isinstance(layer, layers.Conv2D):\n",
    "        filters, biases = layer.get_weights()\n",
    "        print(f\"\\nLayer: {layer.name}, Filter shape: {filters.shape}\")\n",
    "\n",
    "        # YOUR CODE HERE ‚Äî normalize and display filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baX9sCQOXws4"
   },
   "source": [
    "## Part 3C: Activation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SW5D5FvqXws4"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TODO: Create an activation heatmap\n",
    "# ============================================================\n",
    "# This shows WHERE the CNN focuses in the image.\n",
    "# Steps:\n",
    "#   1. Get feature maps from the LAST Conv layer for fire image\n",
    "#   2. Average across all filter channels ‚Üí (H, W) heatmap\n",
    "#   3. Normalize to [0, 1]\n",
    "#   4. Resize to image size using tf.image.resize()\n",
    "#   5. Plot: original, heatmap, and overlay (original + heatmap with alpha=0.4)\n",
    "\n",
    "# Get feature maps for both images\n",
    "fmaps_fire = feature_map_model.predict(tf.expand_dims(test_fire, 0), verbose=0)\n",
    "fmaps_nofire = feature_map_model.predict(tf.expand_dims(test_nofire, 0), verbose=0)\n",
    "\n",
    "# Get last layer's feature maps\n",
    "fmaps_last = fmaps_fire[-1]  # shape: (1, H, W, 128)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# heatmap = np.mean(fmaps_last[0], axis=-1)  # Average across filters\n",
    "# Normalize, resize, and plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Efiu8LRPXws4"
   },
   "source": [
    "## ‚úÖ Task 3 Checkpoint ‚Äî Show your instructor\n",
    "\n",
    "| Criteria | Points |\n",
    "|----------|--------|\n",
    "| Feature maps + filters visualized, heatmap shown, analysis answered | 5 pts |\n",
    "| One visualization completed with partial analysis | 3 pts |\n",
    "| Attempted but visualization not working | 1 pt |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYcfj8VIXws4"
   },
   "source": [
    "### üìù Task 3 Analysis Questions\n",
    "\n",
    "**Q1:** Look at the feature maps from Layer 1 vs Layer 3. How do they differ? What does this tell you about hierarchical feature learning?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q2:** Compare the activation heatmap for the fire image vs the no-fire image. Does the CNN focus on different regions? Does this make sense?\n",
    "\n",
    "*Your answer:*\n",
    "\n",
    "\n",
    "**Q3:** Look at the learned filters from the first Conv layer. Can you identify any edge detectors or color detectors?\n",
    "\n",
    "*Your answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjI3K3loXws5"
   },
   "source": [
    "---\n",
    "\n",
    "# üéÅ BONUS: Grad-CAM (Optional ‚Äî for early finishers)\n",
    "\n",
    "Grad-CAM uses gradients to weight each filter's importance, creating a more principled heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZ8UAVKDXws5"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BONUS: Grad-CAM implementation\n",
    "# ============================================================\n",
    "\n",
    "def make_gradcam_heatmap(img_tensor, model, last_conv_layer_name):\n",
    "    \"\"\"Generate Grad-CAM heatmap for a given image and model.\"\"\"\n",
    "    # Rebuild as Functional model (Keras 3.x compatible)\n",
    "    inp = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = inp\n",
    "    conv_output = None\n",
    "    for layer in model.layers:\n",
    "        x = layer(x)\n",
    "        if layer.name == last_conv_layer_name:\n",
    "            conv_output = x\n",
    "    grad_model = models.Model(inputs=inp, outputs=[conv_output, x])\n",
    "\n",
    "    # Compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_tensor)\n",
    "        predicted_class = predictions[0]\n",
    "\n",
    "    grads = tape.gradient(predicted_class, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-8)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Get the name of the last Conv layer\n",
    "last_conv_name = [l.name for l in model_basic.layers if isinstance(l, layers.Conv2D)][-1]\n",
    "print(f\"Last Conv layer: {last_conv_name}\")\n",
    "\n",
    "# Generate Grad-CAM for fire image\n",
    "heatmap = make_gradcam_heatmap(\n",
    "    tf.expand_dims(test_fire, 0),\n",
    "    model_basic,\n",
    "    last_conv_name\n",
    ")\n",
    "\n",
    "# Resize and overlay\n",
    "heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], (IMG_SIZE, IMG_SIZE)).numpy()[:, :, 0]\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(14, 4))\n",
    "ax1.imshow(test_fire.numpy().astype('uint8'))\n",
    "ax1.set_title('Original', fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(heatmap_resized, cmap='jet')\n",
    "ax2.set_title('Grad-CAM Heatmap', fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "ax3.imshow(test_fire.numpy().astype('uint8'))\n",
    "ax3.imshow(heatmap_resized, cmap='jet', alpha=0.4)\n",
    "ax3.set_title('Grad-CAM Overlay', fontweight='bold')\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.suptitle('Grad-CAM: Where the CNN looks to decide \"Fire\"', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Grad-CAM uses gradients to weight which filters matter most.\")\n",
    "print(\"This is more principled than simply averaging all feature maps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zy1OgDgXws5"
   },
   "source": [
    "---\n",
    "\n",
    "# üéì Session Complete!\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. CNNs preserve spatial structure ‚Äî unlike MLPs that flatten everything\n",
    "2. Convolution + pooling = efficient feature extraction with fewer parameters\n",
    "3. Data augmentation reduces overfitting by creating virtual training variety\n",
    "4. CNNs learn hierarchically: edges ‚Üí textures ‚Üí objects\n",
    "5. Visualization builds understanding and trust in what the model learns\n",
    "\n",
    "**Next Week:** Transfer Learning ‚Äî use models pre-trained on 1.2 MILLION ImageNet images!\n",
    "\n",
    "**Mini Project 5** is now posted on D2L. Due: Week 7.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
